# Очередь

`Queue (очередь)` – является основным примитивом GCD. Очередь представляет собой сущность, выполняющую задачи, поступающие на вход, на одном или множестве потоков. Представьте себе очередь на кассу в любом продуктовом магазине. В данном случае касса, которая вас обслужит – это поток, вы – сама задача, а все вместе – очередь.
Очередь работает по принципу FIFO, таким образом первая задача на очереди будет первой направлена на выполнение на потоке.

![image](https://user-images.githubusercontent.com/47610132/201191698-45fb99f8-0c0e-4da4-bf1c-2ffdbce969bc.png)

> Говоря об очередях, очень хочется заострить внимание на следующем. Очередь (queue) и поток(thread) – не одно и то же. Очередь использует поток или несколько потоков для выполнения поступающих к ней задач.

Очереди делятся на 2 типа:
- `serial (последовательная)` – выполняет задачи последовательно (поочередно). До тех пор, пока задача не будет выполнена, поток не приступит к выполнения следующей задачи в очереди.
- `concurrent (параллельная)` – выполняет задачи параллельно. Задачи, поступающие в concurrent очередь, могут выполняться одновременно на разных потоках.

![image](https://user-images.githubusercontent.com/47610132/182394517-dba6c74c-48fd-4231-a7e0-e09f1f92dcc6.png)

`Context switch`
Поговорим немного про ресурсозатратность. Concurrent очередь достигает возможность параллить задачи благодаря множеству потоков, на которых она выполняет эти самые задачи. У всего есть своя цена и concurrent queue не исключение, процесс переключения между потоками является одним из самых ресурсозатратных в многопоточной среде, а имя ему [context switch.](https://en.wikipedia.org/wiki/Context_switch)

> Переключение контекста (англ. context switch) — в [многозадачных](https://ru.wikipedia.org/wiki/Многозадачность) ОС и средах - процесс прекращения выполнения процессором одной задачи (процесса, потока, нити) с сохранением всей необходимой информации и состояния, необходимых для последующего продолжения с прерванного места, и восстановления и загрузки состояния задачи, к выполнению которой переходит процессор.

Не смотря на то, что context switch оптимизирован на уровне ОС, он все равно требует больших вычислительных ресурсов. Эти ресурсы в основном тратятся на сохранение контекста текущего процесса (что на самом деле задействовано в переключении контекста, зависит от архитектуры, операционной системы и количества совместно используемых ресурсов). В отличии от concurrent, serial очередь использует единственный поток, таким образом выполнение задач в очереди не приводит к context switch.

# Многопоточность vs aсинхронность. Чем отличаются?

`Многопоточность` - каждый поток имеет свой стек и планируется на исполнение отдельно kernel’ом. Поток может общаться с другими потоками. 
Все потоки находятся в общем адресном пространстве приложения и делят одну и ту же виртуальную память и имеют те же права доступа что и процесс приложения. 
Асинхронные событиямя выполняются независимо от основного потока в неблокирующем режиме, что позволяет основному потоку программы продолжить обработку. 
Т.е результат работы функции приходит не сразу после вызова, а когда-нибудь потом.

`Асинхронность` говорит о порядке исполнения кода. Если вызываемая функция не возвращает значение сразу, а отдаёт управление вызывающему коду с обещанием 
выдать значение позже, то эта функция асинхронная. При этом нет никаких предположений о том, как это значение будет считаться: параллельно или нет.

`Процесс` - абстракция, описывающая выполняющуюся программу, общаются друг с другом через передачу сообщений.

`Потоки` - мини - процессы внутри процесса, общаются через общую память в процессе.

- `Main thread / UI Thread`: это поток, который запускается вместе с приложением, 
предварительно определенный последовательный **(serial)** поток. Он слушает взаимодействие с пользователем и изменения пользовательского интерфейса. 
На все изменения немедленно нужна реакция. Будьте осторожны, чтобы не добавлять в эту ветку огромную работу, так как приложение может зависнуть.
> Важно запомнить, что всю отрисовку пользовательского интерфейса мы делаем на `главном потоке!`. Это делается с той точки зрения, что у главного потока
есть гарантированный [Runloop]().

![image](https://user-images.githubusercontent.com/47610132/182425715-7104045c-739a-4c45-8308-1e3bcfdaae80.png)

> Длительная задача в потоке пользовательского интерфейса (неправильно и не следует так делать)

- Background Thread (global): В основном мы создаем задачи в новых потоках исходя из наших потребностей. 
Например, если нам нужно загрузить какое-то изображение большого размера - это делается в фоновом потоке. 
Или любой вызов API. Мы не хотим запрещать пользователям ждать завершения этой задачи. Мы вызовем вызов API для получения списка данных о фильмах в фоновом потоке. 
Когда он прибывает и выполняется синтаксический анализ, мы переключаемся и обновляем пользовательский интерфейс в основном потоке.

- `Синхронная` операция начинает выполнятся сразу при вызове, блокирует поток. Выполняется в текущем потоке.
- `Асинхронная` операция ставит задачу в очередь выполнения, продолжает выполнение кода, из которого вызвана задача. 
Если очередь однопоточная, то задача будет выполнятся после выполнения всех задач, которые уже поставлены в очередь,
если многопоточная - возможно ее выполнение в другом потоке.

# GCD (Grand Central Dispatch)

GCD — это низкоуровневый многопоточный интерфейс от Apple для поддержки параллельного выполнения кода на многоядерном устройстве. 
GCD позволяет вашему девайсу загружать видео в фоновом режиме, сохраняя при этом отзывчивость пользовательского интерфейса.

> DispatchQueue — это объект, который управляет выполнением задач последовательно или параллельно на главном или фоновом потоке. 
Ниже QoS представлены по степени важности, от наиважной и ниже по приоритету.

[Quality of service](https://developer.apple.com/documentation/dispatch/dispatchqos) - приоритет выполнения, применяемый к задачам.
1) `.userInteractive`: Интерактивный пользовательский класс (user interactive) представляет задачи, которые необходимо сделать немедленно. 
Используйте его для обновления пользовательского интерфейса, обработки событий или небольших работ, которые должны быть выполнены с небольшими задержками. 
Общий объем работ, сделанный в этом классе во время выполнения вашего приложения, должен быть небольшим.

2) `.userInitiated`: Инициированный пользовательский класс представляет задачи, которые инициируются из пользовательского интерфейса и могут быть выполнены 
асинхронно. Его нужно использовать, когда пользователь ждет немедленных результатов, и для задач, требующих продолжения взаимодействия с пользователем.

3) `.default`: Дефолтная степень важности.

4) `.utility`: Класс Утилит (utility) представляет длительные по исполнению задачи, как правило, с видимым для пользователя индикатором загрузки. 
Используйте его для вычислений, I/O, при работе с сетями, непрерывных каналов передачи данных и подобных друг другу задач. 
Этот класс является энергоэффективным, к тому же имеет низкое энергопотребление.

5) `.background`: Класс background представляет задачи, о которых пользователь может не знать напрямую. 
Используйте его для предварительной выборки, технического обслуживания и других задач, которые не требуют взаимодействия с пользователем и не 
требовательны ко времени исполнения. Помните, что Apple API-интерфейсы также использует глобальные очереди отправки, так что задачи, 
которые вы добавляете, не будут единственными в этих очередях.

6) `.unspecified`: Представляет собой отсутствие qos и указывает системе на для резерва в будущем.

Всего получается 6 `QoS`ов, из них применимых 5.

# DispatchGroup

Зачастую нам нужно запустить несколько асинхронных процессов, но нам нужно только одно событие, когда все будут завершены. 
Этого можно достичь с помощью DispatchGroup.

> «Группа задач, которые вы отслеживаете как единое целое», — Apple Docs.

Например, иногда вам нужно сделать несколько вызовов API в фоновом потоке. 
Прежде чем приложение будет готово к взаимодействию с пользователем или к обновлению пользовательского интерфейса в основном потоке. Вот код:
```
// 1. Создаем Dispatch Group
let group = DispatchGroup()

// 2.a Task 1
group.enter()
runLongRunningTask1(completion: {
    print("DispatchGroup: Long running Task 1 finished")
    group.leave()
})

// 2.b Task 2
group.enter()
runLongRunningTask2(completion: {
    print("DispatchGroup: Long running Task 2 finished")
    group.leave()
})

// 2.c Task 3
group.enter()
runLongRunningTask3(completion: {
    print("DispatchGroup: Long running Task 3 finished")
    group.leave()
})

// 3. Когда все задачи завершены, уведомляем
let queueType = DispatchQueue.global(qos: .userInitiated)
group.notify(queue: queueType) {
    print("DispatchGroup - notify: All task Finished.")
}
```

- Создаём DispatchGroup
- Затем для этой группы необходимо вызвать событие `group.enter()` для каждой запущенной задачи.
- Для каждой `group.enter()` необходимо также вызывать `group.leave()`, когда задача завершена.
- Когда все пары enter-leave завершены, вызывается `group.notify`.
Если вы заметили, что это делается в фоновом потоке. Вы можете настроить в соответствии с вашими потребностями.

# Semaphore

Семафор позволяет выполнять какой-либо участок кода одновременно только конкретному количеству потоков. 
В основе семафора лежит счетчик, который и определяет, можно ли выполнять участок кода текущему потоку или нет. 
Если счетчик больше нуля — поток выполняет код, в противном случае — нет. В GCD выглядит так:

- `semaphore_create`:   создание семафора (аналог sem_init)
- `semaphore_destroy`: удаление, соответственно (аналог sem_destroy)
- `semaphore_wait`:    блокирующее ожидание на семафоре (аналог sem_wait)
- `semaphore_signal`:   освобождение семафора (аналог sem_post)

```
let semaphore = DispatchSemaphore(value: 1)
DispatchQueue.global().async {
    semaphore.wait()
    sleep(1) // Person 1 playing with Switch
    print("Person 1 - done with Switch")
    semaphore.signal()
}

DispatchQueue.global().async {
    semaphore.wait()
    print("Person 2 - wait finished")
    sleep(1) // Person 2 playing with Switch
    print("Person 2 - done with Switch")
    semaphore.signal()
}
```

Методы **signal** и **wait** работают по принципу инкрементирования / декрементирования внутреннего каунтера семафора (аналогично рекурсивному mutex). Это означает, что поток будет разблокирован только тогда, когда каунтер равен значению value, которое мы передаем в инициализатор.

[Разница между семафором и мьютексом](https://www.youtube.com/watch?v=8wcuLCvMmF8)

# Mutex

Mutex — примитив синхронизации, позволяющий захватить ресурс. Подразумевается, что как только поток обратиться к ресурсу, захваченному мьютексом, никакой другой поток не сможет с ним взаимодействовать до тех пор, пока текущий поток не освободит этот ресурс
Рассмотрим пример использования pthread mutex:

```
var mutex = pthread_mutex_t()
pthread_mutex_init(&mutex, nil)

func doSomething() {
    // Захватываем ресурс
    pthread_mutex_lock(&mutex)

    // Выполняем работу   
    print("Hello World!")

    // Освобождаем ресурс
    pthread_mutex_unlock(&mutex)
}
```

Стоит отметить, что mutex работает по принципу `FIFO`, то есть потоки будут захватывать ресурс по освобождению в том порядке, в котором данные потоки обратились к ресурсу.

**NSLock**
NSLock - более удобная реализация базового mutex из фреймворка Foundation.
Рассмотрим пример использования NSLock:
```
let lock = NSLock()

func doSomething() {
    lock.lock()

    // Выполняем работу
    print("Hello World")

    lock.unlock()
}
```

**Reqursive mutex**
Reqursive mutex — разновидность базового mutex, которая позволяет потоку захватывать ресурс множество раз до тех пор, пока он не освободит его. Ядро операционной системы сохраняет след потока, который захватил ресурс и позволяет ему захватывать ресурс повторно. Рекурсивный мьютекс считает количество блокировок и разблокировок, таким образом ресурс будет захвачен до тех пор, пока их количество не станет равно друг другу. Чаще всего используется в рекурсивных функциях.

```
var mutex = pthread_mutex_t()
// Создаем переменную атрибутов мьютекса
var mutexAttributes = pthread_mutexattr_t()
pthread_mutexattr_init(&mutexAttributes)
// Сетим рекурсивный тип мьютекса
pthread_mutexattr_settype(&mutexAttributes, PTHREAD_MUTEX_RECURSIVE)
// Инициализируем мьютекс с атрибутами
pthread_mutex_init(&mutex, &mutexAttributes)

func doSomething1() {
    pthread_mutex_lock(&mutex)
    doSomething2()
    pthread_mutex_unlock(&mutex)
}

func doSomething2() {
    pthread_mutex_lock(&mutex)
    print("Hello World!")
    pthread_mutex_unlock(&mutex)
}
```

Если бы в данном примере использовался обычный mutex, поток бы бесконечно ожидал, пока он же сам не освободит ресурс.

**NSRecursiveLock**
NSRecursiveLock — более удобная реализация reqursive mutex из фреймворка Foundation.
Рассмотрим пример использования NSRecursiveLock:
let recursiveLock = NSRecursiveLock()

```
func doSomething1() {
    recursiveLock.lock()
    doSomething2()
    recursiveLock.unlock()
}

func doSomething2() {
    recursiveLock.lock()
    print("Hello World!")
    recursiveLock.unlock()
}
```

**Condition**
Condition - еще один примитив синхронизации. Задача, закрытая condition, не начнет свое выполнение до тех пор, пока не получит сигнал из другого потока. Сигнал является неким триггером для condition, который говорит о том, что поток должен выйти из состояния ожидания.
Рассмотрим пример использования condition:

```
// Создаем переменную condition
var condition = pthread_cond_t()
var mutex = pthread_mutex_t()

// Создаем булевый предикат
var booleanPredicate = false

// Инициализируем condition
pthread_cond_init(&condition, nil)
pthread_mutex_init(&mutex, nil)

func doSomething1() {
    pthread_mutex_lock(&mutex)
    
    // Проверяем булевой предикат
    while !booleanPredicate {
        // Переход в состояние ожидания
        pthread_cond_wait(&condition, &mutex)
    }
    
    // Выполняем работу
    print("Hello World!")
    
    pthread_mutex_unlock(&mutex)
}

func doSomething2() {
    pthread_mutex_lock(&mutex)
    
    booleanPredicate = true
    // Выпускаем сигнал в condition
    pthread_cond_signal(&condition)
    
    pthread_mutex_unlock(&mutex)
}
```

**NSCondition**
NSCondition — более удобная реализация condition из фреймворка Foundation. Удобство заключается в том, что используя NSCondition, в отличии от pthread_cond_t, у нас нет необходимости дополнительно создавать mutex, так как NSCondition самостоятельно поддерживает методы **lock()** и **unlock()**.

Рассмотрим пример использования NSCondition:

```
// Создаем булевый предикат
var boolPredicate = false

// Создаем condition
let condition = NSCondition()

func test1() {
    condition.lock()
    
    // Проверяем булевой предикат
    while(!boolPredicate) {
        // Переход в состояние ожидания
        condition.wait()
    }
    
    // Выполняем работу
    print("Hello World!")
    
    condition.unlock()
}

func test2() {
    condition.lock()
    
    boolPredicate = true
    // Выпускаем сигнал в condition
    condition.signal()
    
    condition.unlock()
}
```

**Read Write Lock**

Read Write Lock — примитив синхронизации, который предоставляет потоку доступ к ресурсу на чтение, в это время закрывая возможность записи в ресурс из других потоков.

![image](https://user-images.githubusercontent.com/47610132/201214786-691681be-c04c-4b01-a437-73ead3f5db69.png)

Необходимость использовать **rwlock** появляется тогда, когда много потоков читают данные, и только один поток их пишет [(Reader-writers problem)](https://en.wikipedia.org/wiki/Readers–writers_problem). На первый взгляд кажется, что данную проблему можно легко решить простым mutex, однако этот подход будет требовать больше ресурсов, нежели простой rwlock, так как фактически нет необходимости блокировать доступ к ресурсу полностью. rwlock имеет достаточно простое API:

```
class RWLock {
    // Создаем rwlock
    var lock = pthread_rwlock_t()
    // Создаем rwlock аттрибуты
    var attr = pthread_rwlockattr_t()
    
    // Создаем ресурс
    private var resource: Int = 0
    
    init() {
        // Инициализируем rwlock
        pthread_rwlock_init(&lock, &attr)
    }
    
    
    var testProperty: Int {
        get {
            // Блокируем ресурс на чтение
            pthread_rwlock_rdlock(&lock)
            
            // Создаем временную переменную
            let tmp = resource
            
            // Освобождаем ресурс
            pthread_rwlock_unlock(&lock)
            
            return tmp
        }
        
        set {
            // Блокируем ресурс на запись
            pthread_rwlock_wrlock(&lock)
            
            // Записываем ресурс, гарантируя, что в данный момент времени он не будет перезаписан из другого потока
            resource = newValue
            
            // Освобождаем ресурс
            pthread_rwlock_unlock(&lock)
        }
    }
}
```



# DispatchWorkItem

DispatchWorkItem инкапсулирует работу, которая должна быть выполнена в **dispatch queue** или **dispatch group**. 
DispatchWorkItem, в основном, используется в сценариях, где нам требуется возможность задержки или отмены выполнения блока кода.

> Это позволяет нам отменить поставленную в очередь задачу, однако мы можем `отменить задачу только до того`, как она достигнет начала очереди и начнет выполняться.

- Пример использования:
Представьте, что мы разрабатываем приложение для электронной коммерции, и у нас есть панель поиска, которая показывает результаты с опережением ввода.
Чтобы не отправлять запрос на каждую введенную букву в поисковую панель, мы отправляем только тогда, когда пользователь ничего не печатает в течении 30 миллисекунд.

```
class Controller {
    var workItem: DispatchWorkItem?
    
    func getSearchResult(query: String?) {
        workItem?.cancel()
        
        let networkItem = DispatchWorkItem {
            print("Отправляю запрос на сервер для \(String(describing: query))")
        }
        
        workItem = networkItem
        
        DispatchQueue.global().asyncAfter(
            deadline: .now() + .seconds(2),
            execute: networkItem
        )
    }
}

let controller = Controller()
controller.getSearchResult(query: "Hello")
```

- `Notify` планирует выполнение `work item`  после завершения текущего `work item`.
```
class Controller {
    func getSomeThingFromServer() {
        let networkItem = DispatchWorkItem {
            print("Получаю данные от сервера")
            // Асинхронная задача для получения данных
        }
        
        let notifyTheView = DispatchWorkItem {
            print("Отправляю данные во view элемент")
        }
        
        // Это выполнится когда `networkItem` завершит свою работу
        networkItem.notify(queue: .main) {
            notifyTheView.perform()
        }
        DispatchQueue.global().async(execute: networkItem)
    }
}

let controller = Controller()
controller.getSomeThingFromServer()
```
Это можно использовать для последовательного выполнения двух или более задач, когда первая задача должна быть завершена до выполнения следующей. 
Метод `perform()` для `DispatchWorkItem` запустит выполнение рабочего элемента синхронно в текущем потоке.

- `Wait()` заставляет вызывающую сторону ожидать синхронно, пока `dispatch work item` не завершит выполнение.
```
class Controller {
    func getSomeThingFromServer() {
        let networkItem = DispatchWorkItem {
            print("Получаю данные от сервера")
            print(Thread.current)
        }
        
        DispatchQueue.global().async(execute: networkItem)
        
        // Блокирует текущий поток пока `networkItem` не завершит свою работу
        networkItem.wait()
        
        // Выводит принт, после того как `networkItem` завершит свою работу
        print("Завершил работу")
    }
}

let controller = Controller()
controller.getSomeThingFromServer()

Получаю данные от сервера
<NSThread: 0x600000f5c840>{number = 4, name = (null)}
Завершил работу
```

- `DispatchWorkItemFlags` определяет набор действий для `work item`, как `QoS` и необходимость создания барьера или порождение нового, отдельного, потока. 
Наиболее часто используемые флаги — `assignCurrentContext` и `barrier`.

`barrier` - Заставляет `work item` действовать как барьерный блок при отправке в параллельную очередь. 
В параллельной очереди несколько задач выполняются одновременно в разных потоках.

Когда `work item` с флагом `barrier` начинает выполняться, все задачи в очереди временно приостанавливаются и будут возобновлены после завершения этого рабочего элемента.

`assignCurrentContext` - Задает атрибуты для `work item` в соответствии с атрибутами текущего контекста выполнения.

```
func barrierExample() {
    let concurrentQueue = DispatchQueue(label: "barriers", attributes: .concurrent)
    
    for a in 1...3 {
        concurrentQueue.async {
            print("Асинхронная задача \(a)")
        }
    }
    
    for b in 4...6 {
        concurrentQueue.async(flags: .barrier) {
            print("Барьер номер \(b)")
        }
    }
    
    for c in 7...10 {
        concurrentQueue.async {
            print("Завершение задач \(c)")
        }
    }
}

barrierExample()

Асинхронная задача 1
Асинхронная задача 2
Асинхронная задача 3
Барьер номер 4
Барьер номер 5
Барьер номер 6
Завершение задач 7
Завершение задач 8
Завершение задач 9
Завершение задач 10
```

Когда мы добавляем барьер в параллельную очередь, она откладывает выполнение задачи, помеченной барьером (и все остальные, которые поступят в очередь во время выполнения такой задачи), до тех пор, пока все предыдущие задачи не будут выполнены. После того, как все предудщие задачи будут выполнены, очередь выполнит задачу, помеченную барьером самостоятельно. Как только задача с барьером будет выполнена, очередь вернется к своему нормальному режиму работы.

![image](https://user-images.githubusercontent.com/47610132/201207354-d7a45ad7-3f85-4f28-913a-efe95f6b832b.png)


# Operation

`Operation` - представляет собой законченную задачу и является абстрактным классом, который предоставляет вам потока-безопасную структуру для моделирования состояния операции, ее приоритета, зависимостей (dependencies) от других Operations и управления этой операцией. Operation, в отличии от GCD можно принудительно остановить. OperationQueue - класс, объединяющий наши задачи в единый объект, в котором происходит управление их жизненным циклом. Очередь управляет помещенными задачами на основе их приоритета и готовности.

Очень важное свойство `maxConcurrentOperationCount` задает количество одновременно выполняемых операции на этой очереди и, задавая его равным 1, мы устанавливаем **последовательную(serial)** очередь операций.
По умолчанию значение свойства maxConcurrentOperationCount устанавливается равным default, что означает максимально возможное число одновременно выполняемых операций:

![image](https://user-images.githubusercontent.com/47610132/201190188-e2ff5404-a52e-4d27-ad86-4387fac66817.png)
![image](https://user-images.githubusercontent.com/47610132/201213599-aebec493-4052-4be6-b559-5802f381a067.png)

`Жизненный цикл:`

![image](https://user-images.githubusercontent.com/47610132/201211919-7cd3753a-04d5-4f5f-a86d-051293edc4a0.png)

За период своего существования Operations проходит через разные этапы. При добавлении в очередь она находится в состоянии ожидания. В этом состоянии она ожидает своих условий. Как только все они будут выполнены, Operations переходит в состояние готовности, и в случае наличия открытого слота она начнет выполняться. Выполнив всю свою работу, Operations войдет в состояние Finished, и будет удалена из OperationQueue. В каждом состоянии (кроме Завершенного) Operation может быть отменена.

`Отмена`

Отмена Operation довольно проста. В зависимости от операции отмена может иметь совершенно разные значения. Например, при запуске сетевого запроса отмена может привести к остановке этого запроса. При импорте данных это может означать отказ от транзакции. Ответственность за назначение этого значения лежит на вас.

Итак, как отменить Operation? Вы просто вызываете метод .cancel (). Это приведет к изменению свойства isCancelled. Это все, что сделает iOS для вас. От вас зависит, как реагировать на эту отмену операции и как вести себя дальше.

```
let op = DemoOperation()
OperationQueue.addOperations([op], waitUntilFinished: false)

op.cancel()
```

Имейте в виду, что отмена операции приводит к отмене всех ее условий и немедленному запуску, чтобы как можно скорее войти в состояние Finished. Переход в состояние Finished — это единственный путь, чтобы удалить операцию из очереди.

Если вы забыли проверить на отмену операции, вы можете увидеть, что они выполняются, даже если вы их отменили. Также имейте в виду, что это восприимчиво к «race condition». Нажатие кнопки и установка отметки занимает несколько микросекунд. В течение этого времени операция может завершиться и отметка об отмене операции не будет иметь никакого эффекта.

`Готовность`

Готовность описывается только одним Boolean значением. Это означает, что операция готова к выполнению, и она ждет очереди своего запуска. В последовательной очереди сначала выполняется операция, которая входит в состояние «Готовности», хотя она и может находиться на позиции 9 в очереди. Если в состояние готовности вошли одновременно несколько операций, будет выполнена приоритизация их выполнения. Operation войдет в состояние готовности только после завершения всех ее зависимостей.

# Проблемы 

Многопоточность предназначена для решения проблем, но как и любые другие технологии может порождать новые. В большинстве случаев проблемы связанны с доступом к ресурсам. Самые распространенные из них:

# `Race conditions`
> Состояние гонки — это нежелательная ситуация, возникающая, когда система пытается выполнить две или более операций одновременно. Особенно вы можете пострадать, когда процессы или потоки завист от некоторого, общего, состояния.

Представьте, что мы продаем айфоны в двух разных магазинах приложений, один в США, а другой на Тайване. И сейчас на складе всего 2000 айфонов. Мы должны подтвердить остаток на складе, прежде чем телефоны будут проданы.

```
struct IPhone {
    static var stock = 2000
}

class AppleStore {
    let location: String
    
    init(location: String) {
        self.location = location
    }
    
    func sell(value: Int) {
        print("\(location): start transaction process...")
        if IPhone.stock > value {
            Thread.sleep(forTimeInterval: Double.random(in: 0...1))
            IPhone.stock -= value
            
            print("\(location): \(value) has been sold")
            print("current balance is \(IPhone.stock)")
            
            if IPhone.stock < 0 {
                print("there is a stock issue")
            }
        } else {
            print("\(location): Can't sell due to insufficent balance")
        }
    }
}

func tryRaceCondition() {
    let appleStoreUS = AppleStore(location: "US")
    let appleStoreTW = AppleStore(location: "TW")
    
    let queue = DispatchQueue(label: "sellQueue", attributes: .concurrent)
    
    queue.async {
        appleStoreUS.sell(value: 1000)
    }
    
    queue.async {
        appleStoreTW.sell(value: 1500)
    }
}

tryRaceCondition()
```

После запуска мы обнаружим, что все идет не так, как ожидалось. Наш покупатель смог купить на несколько iPhone больше, чем осталось на складе!
И мы смогли понять сценарий того, как это произошло, благодаря логу ниже.

![image](https://user-images.githubusercontent.com/47610132/187022744-6b7feec0-d008-43d3-9c3a-80c25b35adfa.png)

Так бывает, когда несколько магазинов берут свои товары из одного склада. Проблема в том, что когда один магазин проверял баланс, его было достаточно, но в момент списания, баланс, уже был изменен другим магазином. В результате этого стало недостаточно.

Вот как работает состояние гонки.

# `Data Race`
> Гонка данных возникает, когда 2 или более потоков пытаются одновременно асинхронно получить доступ (чтение/запись) к одной и той же ячейке памяти.

**Race condition** может возникнуть без **data race**, в то время как **data race** может произойти без **race condition**. Например, порядок событий может быть согласованным, но если чтение всегда происходит одновременно с записью, **data race** все равно будет.

К примеру, у нас есть класс `Counter`, где функция `addCount` увеличивает `count`, при каждом вызове, на 1.

```
class Counter {
    
    var count = 0
    
    func addCount() {
        count += 1
    }
}
```

```
let totalCount = 1000
let counter = Counter()
let group = DispatchGroup()

// Вызываем `addCount()` асинхронно 1000 раз
for _ in 0..<totalCount {
    
    DispatchQueue.global().async(group: group) {
        self.counter.addCount()
    }
}

group.notify(queue: .main) {
    // Диспатч груп завершился
    // Показываем `count` в лейбле
    self.statusLabel.text = "\(self.counter.count)"
}
```

Приведенный выше код вызывает функцию `addCount()` 1000 раз асинхронно, используя диспатч груп. Как только выполнение группы отправки будет завершено, мы покажем значение счетчика в лейбле. В идеале мы должны видеть 1000 в лейблей каждый раз, когда мы нажимаем на кнопку, однако это не так. Результат, который мы получаем, противоречив, время от времени мы можем получать 1000, но очень часто получаемые значения меньше 1000.

Как вы могли догадаться, это несоответствие вызвано гонкой данных. Когда несколько потоков пытаются асинхронно получить доступ к **count**, нет гарантии, что каждый поток будет обновлять значение **count** один за другим. Это приводит к тому, что окончательные результаты мы получаем крайне непоследовательными и очень трудно предсказуемыми.


В Xcode есть Thread Sanitizer, который помогает разработчикам обнаруживать гонки данных более последовательным образом. Чтобы его включить, нужно открыть `Product > Scheme > Edit Scheme…` После этого, внутри "изменить схему" выбрать `Run > Diagnostic` и выбрать `Thread Sanitizer`.

![image](https://user-images.githubusercontent.com/47610132/187060183-e39be926-8b53-4242-a1c7-b6d089b061b6.png)

Теперь, когда мы знаем, как происходит гонка данных, то что, если мы сделаем то же самое, используя `async/await` и асинхронные задачи... Произойдет ли гонка данных? Давайте выясним!

```
let totalCount = 1000
let counter = Counter()

Task {
            await withTaskGroup(
                of: Void.self, body: { taskGroup in
                    for _ in 0..<totalCount {
                        taskGroup.addTask {
                            self.counter.addCount()
                        }
                    }
                }
            )
            
            print("\(counter.count)")
        }
```

В приведенном выше коде мы используем метод `withTaskGroup(of:body:)` для создания группы задач. Затем в группе задач мы создаем 1000 дочерних задач для асинхронного выполнения функции `addCount()`. Стоит отметить, что метод `withTaskGroup(of:body:)` является ожидаемым, поэтому он будет приостановлен до завершения всех дочерних задач. Как только это произойдет, мы выводим значение счетчика в принтах.

Когда я пытаюсь запустить приведенный выше код, результаты, которые я получаю, на удивление стабильны! Я вижу, как 1000 выводится в дебаг панель каждый раз, когда код завершает выполнение. Означает ли это, что гонок данных не будет, когда мы используем задачи и группы задач? 🤔

На самом деле нет, т.к хкод, при включенном `Thread Sanitizer` предупреждает меня о том, что в определенном участке кода может произойти `data race`.

![image](https://user-images.githubusercontent.com/47610132/187060435-0b072b3f-3c34-4679-a067-65e0280f14a4.png)

Так почему же, если хкод подсказывает нам о наличии гонки данных, то каждый раз когда мы запускаем, код выше, то результат 1000? Я предполагаю, что Apple проделала большую работу по оптимизации всего модуля параллелизма `Swift`, поэтому он может разрешать простые условия гонки данных, подобные тому, что мы имеем в нашем примере кода. При использовании очереди мы можем избежать гонок данных, используя `последовательную (serial)` очередь для предотвращения одновременных операций записи.

# `Deadlocks`
> Cитуация, в которой поток бесконечно ожидает доступ к ресурсу, который никогда не будет освобожден.

Deadlock — это ситуация в многозадачной среде, при которой несколько потоков находятся в состоянии ожидания ресурса, занятого друг другом, и ни один из них не может продолжать свое выполнение. Таким образом оба потока бесконечно ожидая друг друга никогда не выполнят задачу, что может привести к неожиданному поведению приложения.

Банальнаый пример:
```
DispatchQueue.main.sync {
            print("Hello!")
        }
```

# `Инверсия приоритетов`
> Инверсия приоритета происходит, когда очереди с более низким **(quality of service)** присваивается более высокий системный приоритет, чем очередь с более **quality of service**.

```
let high = DispatchQueue.global(qos: .userInteractive)
let medium = DispatchQueue.global(qos: .userInitiated)
let low = DispatchQueue.global(qos: .background)

let semaphore = DispatchSemaphore(value: 1)

high.async {
  // Ждём 2 секунды, чтобы убедиться, что все остальные задачи поставлены в очередь.
  Thread.sleep(forTimeInterval: 2)
            
  self.semaphore.wait()
            
  defer {
    self.semaphore.signal()
  }

  print("Выполняется задача с высоким приоритетом")
}

for i in 1 ... 10 {
  medium.async {
    let waitTime = Double(exactly: arc4random_uniform(7))!
    
    print("Запуск medium задачи\(i)")
    
    Thread.sleep(forTimeInterval: waitTime)
  }
}

low.async {
  self.semaphore.wait()
  
  defer {
    self.semaphore.signal()
  }
  
  print("Выполняется долгая задача с самым низким приоритетом")
  
  Thread.sleep(forTimeInterval: 5)
}
```

И на выходе мы получаем вот такой результат:
`
Запуск medium задачи6
Запуск medium задачи1
Запуск medium задачи5
Запуск medium задачи7
Запуск medium задачи2
Запуск medium задачи3
Запуск medium задачи4
Запуск medium задачи8
Запуск medium задачи9
Запуск medium задачи10
Выполняется долгая задача с самым низким приоритетом
Выполняется задача с высоким приоритетом
`

# Немного дополнительного мнения

Небольшая ремарка, мало кто знает, но вызов `sync` у очереди в большинстве случаев не добавляет задачу в саму очередь, вернее сказать задача ожидает выполнение других задач в очереди, **НО** сама она выполняется на текущем потоке. `Sync` ведь заставляет вызывающий поток ожидать, вот разработчики в целях оптимизации и подумали, чтобы избежать переключение контекста. Иногда лучше сделать вид, что выполняешь задачу в очереди, а на самом деле выполнить её на стороне вызывающего кода. 

Но спустя какое-то время, разработчики ядра улучшили свой планировщик, и с вызовом `sync` пошли проблемы, с одной стороны игнорируется `qos` очереди, так как выполняется не там, с другой - переключение контекста стало не таким трудозатратным делом как раньше.

Поэтому позже разработчики `GCD` добавили метод `asyncAndWait`. Он гарантирует, что задача выполнится внутри самой очереди и гарантирует что вы дождётесь на текущем потоке её выполнения.

Итог: вместо `sync` в 90% лучше использовать `asyncAndWait`. [Спасибо большое Руслану за разъяснение!](https://t.me/jeudesprits_swift_notes)

Не знаю кому это нужно будет, но вот как-то так 😅
Большинство разработчиков использует libdispatch неэффективно из-за того как её представили сообществу, а также из-за запутанной документации и API. Я пришел к этой мысли после чтения обсуждения «concurrency» в рассылке посвященной развитию Swift (swift-evolution). Особенно просвещают сообщения от Пьера Хабузит (Pierre Habouzit — занимается поддержкой libdispatch в Apple):

- Начните с последовательного исполнения (serial). Когда обнаружите проблему с производительностью, сделайте измерения, чтобы выяснить причину. И если параллельное исполнение помогает, осторожно используйте его. Всегда проверяйте работу параллельного кода под давлением со стороны системы. По умолчанию переиспользуйте очереди. Добавляйте очереди тогда, когда это приносит измеряемые преимущества. В большинстве приложений не стоит использовать более трех-четырех очередей.

- В случае параллельного исполнения ваши задачи не должны бороться между собой, в противном случае производительность резко падает. Борьба принимает разные формы. Очевидный случай: борьба за захват блокировки. Но в реальности, такая борьба означает ничто иное, как использование разделяемого ресурса, которое становится узким местом: IPC (межпроцессное взаимодействие) / демоны ОС, malloc (блокировка), разделяемая память, ввод / вывод.

- libdispatch эффективна, но чудес не бывает. Ресурсы не бесконечны. Вы не можете игнорировать реальность ОС и аппаратного обеспечения, на которых исполняется код. Также не всякий код хорошо распараллеливается.

> Вот оригинал: [1](https://twitter.com/pedantcoder/status/1081658384577835009), [2](https://twitter.com/pedantcoder/status/1081659784841969665), [3](https://twitter.com/pedantcoder/status/904839926180569089), [4](https://twitter.com/pedantcoder/status/904840156330344449), [5](https://twitter.com/Catfish_Man/status/1081581652147490817)

- Если Вам нужно отправлять задачи в одну очередь и асинхронно, и синхронно, то вместо `dispatch_sync()` используйте `dispatch_async_and_wait()`. `dispatch_async_and_wait()` не гарантирует исполнение на потоке, с которого произошел вызов, что позволяет уменьшить переключения контекста, когда целевая очередь активна. (Прим. перев. 1: на самом деле dispatch_sync() тоже не гарантирует, в документации про него утверждается лишь «исполняет блок на текущем потоке, всегда когда возможно. С одним исключением: блок отправленный в главную очередь — всегда исполняется на главном потоке.»)
(Прим. перев. 2: о dispatch_async_and_wait() в документации и в исходном коде)

> [Вот оригинал](https://twitter.com/pedantcoder/status/1135938715098857477)

- Фоновый `Quality Of Service` может *никогда* не выполняться - например. режим низкого энергопотребления. 
> On iPhones, discretionary and background operations, including networking, are paused when Low Power Mode is enabled. [Вот документация](https://developer.apple.com/library/archive/documentation/Performance/Conceptual/EnergyGuide-iOS/PrioritizeWorkWithQoS.html)

- async/await + actors: почитать [здесь](https://lists.swift.org/pipermail/swift-evolution/Week-of-Mon-20170828/039410.html) и [здесь](https://lists.swift.org/pipermail/swift-evolution/Week-of-Mon-20170828/039429.html)

- Не используйте семафоры для ожидания асинхронной задачи.

> [Вот оригинал](https://twitter.com/pedantcoder/status/1175062243806863360)

- Не используйте async-вызовы для маленьких задач

> [Раз](https://twitter.com/pedantcoder/status/1081657739451891713), [Два](https://twitter.com/pedantcoder/status/1081642189048840192), [Три](https://twitter.com/pedantcoder/status/1081642631732457472), [Четыре](https://twitter.com/pedantcoder/status/1081648778975707136)

# Полезные ресурсы
- [Modernizing Grand Central Dispatch Usage](https://developer.apple.com/videos/play/wwdc2017/706/)
- Ну и само собой `если вы не читали` [вот это](https://gist.github.com/lattner/31ed37682ef1576b16bca1432ea9f782) то точно стоит прочитать как минимум два раза!
- [Common Concurrency Problems](https://pages.cs.wisc.edu/~remzi/OSTEP/threads-bugs.pdf)
